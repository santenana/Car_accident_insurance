{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cars(url,main_div,estructura_1,estructura_2,imagen):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # clase_sup = soup.find_all(\"lo\", {\"class\":'ui-search-layout ui-search-layout--grid'})\n",
    "    divs = soup.find_all(main_div[0], {\"class\":main_div[1]})\n",
    "    # print(divs)\n",
    "    carros = []\n",
    "    for item in divs:\n",
    "        dict_list = {}\n",
    "        clase_alta = BeautifulSoup(str(item),'html.parser')\n",
    "        nombre_carro = clase_alta.find(estructura_1[0], class_=estructura_1[1]).text\n",
    "        precio_carro = clase_alta.find(estructura_2[0], class_=estructura_2[1]).text\n",
    "        imagenes = clase_alta.find_all(imagen[0], class_=imagen[1])\n",
    "        for img in imagenes:\n",
    "            url_imagen = img.get('src')\n",
    "            if not url_imagen.endswith('.webp'):\n",
    "                url_imagen = img.get('data-src')\n",
    "            else:\n",
    "                url_imagen = img.get('src')\n",
    "        dict_list['Nombre'] = nombre_carro\n",
    "        dict_list['Precio'] = precio_carro\n",
    "        dict_list['Imagen_URL'] = url_imagen\n",
    "        carros.append(dict_list)\n",
    "    return carros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://carros.tucarro.com.co/_Desde_529_NoIndex_True'\n",
    "main_div = ['div','andes-card poly-card poly-card--grid-card andes-card--flat andes-card--padding-0 andes-card--animated']\n",
    "estructura_1 = ['h2','poly-box poly-component__title']\n",
    "estructura_2 = ['span','andes-money-amount__fraction']\n",
    "imagen = ['img','poly-component__picture']\n",
    "carros = extract_cars(url,main_div,estructura_1,estructura_2,imagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "carros\n",
    "with open('carros.csv', mode='a', newline='') as file: \n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Nombre_Imagen', 'Precio'])\n",
    "    file.seek(0, os.SEEK_END)  \n",
    "    if file.tell() == 0:\n",
    "        writer.writerow(['Nombre_Imagen', 'Precio']) \n",
    "    for item in carros:\n",
    "        try:\n",
    "            nombre_imagen = item['Nombre'].replace(' ', '_') + '.webp'\n",
    "            ruta_imagen = os.path.join('imagenes_carros', nombre_imagen)\n",
    "            response = requests.get(item['Imagen_URL'])\n",
    "            response.raise_for_status()\n",
    "            with open(ruta_imagen, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            writer.writerow([nombre_imagen, item['Precio']])\n",
    "        except Exception as e:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cars_2(url,main_div,estructura_1,estructura_2,img_src):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # clase_sup = soup.find_all(\"lo\", {\"class\":'ui-search-layout ui-search-layout--grid'})\n",
    "    divs = soup.find_all(main_div[0], {\"class\":main_div[1]})\n",
    "    # print(divs)\n",
    "    carros = []\n",
    "    for item in divs:\n",
    "        dict_list = {}\n",
    "        clase_alta = BeautifulSoup(str(item),'html.parser')\n",
    "        nombre_carro = clase_alta.find(estructura_1[0], itemprop=estructura_1[1]).text\n",
    "        precio_carro = clase_alta.find(estructura_2[0], class_=estructura_2[1]).text\n",
    "        imagenes = clase_alta.find_all(img_src[0], itemprop_=img_src[1])\n",
    "        url_imagen = ''\n",
    "        for img in imagenes:\n",
    "            url_imagen = img.get('src')\n",
    "            if not url_imagen.endswith('.webp'):\n",
    "                url_imagen = img.get('data-src')\n",
    "            else:\n",
    "                url_imagen = img.get('src')\n",
    "        dict_list['Nombre'] = nombre_carro\n",
    "        dict_list['Precio'] = precio_carro\n",
    "        dict_list['Imagen_URL'] = url_imagen\n",
    "        carros.append(dict_list)\n",
    "    return carros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.autocosmos.com.co/catalogo/vigente/audi'\n",
    "main_div = ['section','section m-brand']\n",
    "estructura_1 = ['meta','model']\n",
    "estructura_2 = ['span','model-card__price']\n",
    "img_src = ['img','image']\n",
    "carros_brand = extract_cars_2(url,main_div,estructura_1,estructura_2,img_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Nombre': '',\n",
       "  'Precio': '\\n\\n\\n\\nPrecio de lista a partir de:$155.400.000\\n',\n",
       "  'Imagen_URL': ''}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carros_brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def generate_image_name(img_url):\n",
    "    # Utiliza un hash de la URL para generar un nombre Ãºnico\n",
    "    img_hash = hashlib.md5(img_url.encode()).hexdigest()\n",
    "    return f'{img_hash}.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.google.com/search?sca_esv=116b66c83e832583&hl=es&sxsrf=ADLYWIJnmkObXxD8Q-QL3DZe8bvgvJ2QYg:1728320665401&q=accidentes+carro&udm=2&fbs=AEQNm0CbCVgAZ5mWEJDg6aoPVcBgWizR0-0aFOH11Sb5tlNhdzvguW7TJ8ZJj4v-NOGupFiwZ48Dx3g7oaON1i1EOWFr4Xc7Fgj5H7_b7U_nkofi9rKU-iWnq3akONsyXiC15fc_IBFSshmfGsrcUEcLgDoqs8pLQOIVXv7pyxZ-bFASagYhN_qTShtUJbLXiufJYrtd4AGCg4WlSComD-Fvf5_swv72GA&sa=X&ved=2ahUKEwirvcaa4PyIAxVMfzABHc6lEXkQtKgLegQIFBAB&biw=1592&bih=791&dpr=1.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "\n",
    "save_directory = '/home/santenana/Proyectos/02_ObjectDetection/Accidentes_carros'\n",
    "\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "images = soup.find_all('img')\n",
    "\n",
    "def get_original_image_url(thumbnail_url):\n",
    "    return thumbnail_url.replace('/tbn:', '/')  \n",
    "\n",
    "for img in images:\n",
    "    img_url = img.get('src') or img.get('data-src')\n",
    "    if img_url:\n",
    "        original_url = get_original_image_url(img_url)  \n",
    "        try:\n",
    "            img_data = requests.get(original_url).content\n",
    "            img_name = os.path.join(save_directory, os.path.basename(original_url))\n",
    "            with open(img_name, 'wb') as f:\n",
    "                f.write(img_data)\n",
    "        except Exception as e:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Images....\n",
      "Found 0 images\n",
      "Start downloading...\n",
      "Download Completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "Google_Image = 'https://www.google.com/?hl=es'\n",
    "\n",
    "u_agnt = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36 Edg/129.0.0.0',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "    'Accept-Encoding': 'none',\n",
    "    'Accept-Language': 'en-US,en;q=0.8',\n",
    "    'Connection': 'keep-alive',}\n",
    "\n",
    "Image_Folder = '/home/santenana/Proyectos/02_ObjectDetection/Accidentes_carros'\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(Image_Folder):\n",
    "        os.mkdir(Image_Folder)\n",
    "    download_images()\n",
    "\n",
    "def download_images():\n",
    "    data = input('Enter your search keyword: ')\n",
    "    num_images = int(input('Enter the number of images you want: '))\n",
    "    print('Searching Images....')\n",
    "    search_url = Google_Image + 'q=' + data \n",
    "    response = requests.get(search_url, headers=u_agnt)\n",
    "    html = response.text \n",
    "    b_soup = BeautifulSoup(html, 'html.parser') \n",
    "    results = b_soup.findAll('img', {'class': 'rg_i Q4LuWd'})\n",
    "    count = 0\n",
    "    imagelinks= []\n",
    "    for res in results:\n",
    "        try:\n",
    "            link = res['data-src']\n",
    "            imagelinks.append(link)\n",
    "            count = count + 1\n",
    "            if (count >= num_images):\n",
    "                break   \n",
    "        except KeyError:\n",
    "            continue\n",
    "    print(f'Found {len(imagelinks)} images')\n",
    "    print('Start downloading...')\n",
    "\n",
    "    for i, imagelink in enumerate(imagelinks):\n",
    "        response = requests.get(imagelink)\n",
    "        imagename = Image_Folder + '/' + data + str(i+1) + '.jpg'\n",
    "        with open(imagename, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "    print('Download Completed!')\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "## Scraping process credits to: https://github.com/goutamborthakur555"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
